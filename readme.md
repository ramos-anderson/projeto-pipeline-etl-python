# Projeto de ETL: Pipeline Automatizado de Dados de Vendas

![Status](https://img.shields.io/badge/Status-ConcluÃ­do-brightgreen?style=for-the-badge) ![Python](https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python) ![SQLite](https://img.shields.io/badge/SQLite-3.3-blue?style=for-the-badge&logo=sqlite)

Este repositÃ³rio contÃ©m o cÃ³digo de um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) completo e **automatizado**, desenvolvido em Python. O objetivo Ã© simular um ambiente de produÃ§Ã£o onde dados de vendas sÃ£o coletados de uma fonte online, processados e carregados em um banco de dados SQL para futuras anÃ¡lises.

Este projeto demonstra habilidades prÃ¡ticas em **Engenharia de Dados**, **automaÃ§Ã£o de processos**, arquitetura de software e boas prÃ¡ticas de organizaÃ§Ã£o de cÃ³digo.

---

## ğŸ“ˆ Arquitetura do Pipeline

O fluxo de dados segue a seguinte arquitetura de ETL, orquestrada por scripts Python:

[Fonte (CSV no GitHub)] --> [1. ExtraÃ§Ã£o (extract.py)] --> [Dados Brutos (data/raw)] --> [2. TransformaÃ§Ã£o (transform.py)] --> [Dados Processados (data/processed)] --> [3. Carga (load.py)] --> [Banco de Dados (database/vendas.db)]

Todo este processo Ã© agendado para rodar diariamente sem intervenÃ§Ã£o humana.

---

## ğŸ› ï¸ Tecnologias Utilizadas

*   **Linguagem:** Python 3
*   **Bibliotecas:**
    *   **Pandas:** Para manipulaÃ§Ã£o e transformaÃ§Ã£o dos dados em dataframes.
    *   **SQLite3:** MÃ³dulo nativo para a criaÃ§Ã£o e gestÃ£o do banco de dados SQL.
*   **Banco de Dados:** SQLite
*   **AutomaÃ§Ã£o e OrquestraÃ§Ã£o:**
    *   **`main.py`:** Script Python que serve como orquestrador, chamando os mÃ³dulos de ETL na ordem correta.
    *   **Agendador de Tarefas do Windows:** Utilizado para **automatizar a execuÃ§Ã£o diÃ¡ria** do pipeline, simulando um ambiente de produÃ§Ã£o real.

---

## ğŸš€ Funcionalidades

*   **ExtraÃ§Ã£o (Extract):** Coleta de forma robusta o dataset de vendas (`olist_orders_dataset.csv`) de um repositÃ³rio pÃºblico no GitHub.
*   **TransformaÃ§Ã£o (Transform):**
    *   Converte colunas de texto para formatos `datetime`, tratando erros de forma programÃ¡tica.
    *   Renomeia colunas para o padrÃ£o do Data Warehouse (`snake_case`).
    *   Cria uma nova mÃ©trica de negÃ³cio (`tempo_aprovacao_horas`) atravÃ©s de engenharia de features, calculando o tempo em horas entre o pedido e a aprovaÃ§Ã£o.
*   **Carga (Load):** Carrega os dados transformados em uma tabela (`pedidos`) dentro de um banco de dados SQLite, utilizando um mÃ©todo de substituiÃ§Ã£o (`if_exists='replace'`) para garantir a atualizaÃ§Ã£o dos dados a cada nova execuÃ§Ã£o.
*   **AutomaÃ§Ã£o:** O pipeline completo foi agendado para **execuÃ§Ã£o diÃ¡ria autÃ´noma**, garantindo que o banco de dados esteja sempre atualizado para as equipes de anÃ¡lise.

---

## ğŸ“ Estrutura do Projeto

O projeto Ã© organizado de forma modular para garantir clareza, manutenibilidade e escalabilidade:

pipeline_vendas_etl/
â”œâ”€â”€ data/ # Armazena os dados (ignorado pelo Git)
â”œâ”€â”€ database/ # Armazena o banco de dados (ignorado pelo Git)
â”œâ”€â”€ scripts/ # CÃ³digo fonte modularizado do pipeline
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â”œâ”€â”€ load.py
â”‚ â””â”€â”€ main.py
â”œâ”€â”€ .gitignore # Define arquivos/pastas ignorados pelo Git
â”œâ”€â”€ requirements.txt # Lista de dependÃªncias Python
â””â”€â”€ README.md # Esta documentaÃ§Ã£o

*ObservaÃ§Ã£o: As pastas `data/` e `database/` sÃ£o geradas dinamicamente e estÃ£o no `.gitignore` pois a boa prÃ¡tica em projetos de pipeline Ã© versionar o cÃ³digo que GERA os dados, e nÃ£o os dados em si.*

---

## âš™ï¸ Como Executar o Projeto

**1. Clone o repositÃ³rio:**
```bash
git clone https://github.com/ramos-anderson/projeto-pipeline-vendas-etl.git
cd projeto-pipeline-vendas-etl

(Nota: Lembre-se de substituir o link acima pelo link correto do seu repositÃ³rio!)

2. Crie e ative um ambiente virtual:

# Criar o ambiente
python -m venv venv

# Ativar no Windows
venv\Scripts\activate

3. Instale as dependÃªncias:

pip install -r requirements.txt

(Nota: VocÃª precisarÃ¡ criar um arquivo requirements.txt com pandas dentro)

4. Execute o pipeline:
Para rodar o processo completo de ETL manualmente, execute o script principal.

python scripts/main.py

Ao final da execuÃ§Ã£o, o banco de dados database/vendas.db estarÃ¡ criado e pronto para ser consultado.

---


